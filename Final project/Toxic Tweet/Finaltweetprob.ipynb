{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e679c08-c97c-4dcc-82c8-38435519ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"D:\\Final project\\Toxic Tweet\\FinalBalancedDataset.csv\\FinalBalancedDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18306c67-af0c-4e44-b6ea-7979476d38a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Toxicity</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56740</th>\n",
       "      <td>56740</td>\n",
       "      <td>1</td>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56741</th>\n",
       "      <td>56741</td>\n",
       "      <td>1</td>\n",
       "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56742</th>\n",
       "      <td>56742</td>\n",
       "      <td>1</td>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56743</th>\n",
       "      <td>56743</td>\n",
       "      <td>1</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56744</th>\n",
       "      <td>56744</td>\n",
       "      <td>0</td>\n",
       "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56745 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Toxicity                                              tweet\n",
       "0               0         0   @user when a father is dysfunctional and is s...\n",
       "1               1         0  @user @user thanks for #lyft credit i can't us...\n",
       "2               2         0                                bihday your majesty\n",
       "3               3         0  #model   i love u take with u all the time in ...\n",
       "4               4         0             factsguide: society now    #motivation\n",
       "...           ...       ...                                                ...\n",
       "56740       56740         1  you's a muthaf***in lie &#8220;@LifeAsKing: @2...\n",
       "56741       56741         1  you've gone and broke the wrong heart baby, an...\n",
       "56742       56742         1  young buck wanna eat!!.. dat nigguh like I ain...\n",
       "56743       56743         1              youu got wild bitches tellin you lies\n",
       "56744       56744         0  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n",
       "\n",
       "[56745 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "693e900b-b069-4915-9a59-0f48b261c24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Toxicity</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56740</th>\n",
       "      <td>1</td>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56741</th>\n",
       "      <td>1</td>\n",
       "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56742</th>\n",
       "      <td>1</td>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56743</th>\n",
       "      <td>1</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56744</th>\n",
       "      <td>0</td>\n",
       "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56745 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Toxicity                                              tweet\n",
       "0             0   @user when a father is dysfunctional and is s...\n",
       "1             0  @user @user thanks for #lyft credit i can't us...\n",
       "2             0                                bihday your majesty\n",
       "3             0  #model   i love u take with u all the time in ...\n",
       "4             0             factsguide: society now    #motivation\n",
       "...         ...                                                ...\n",
       "56740         1  you's a muthaf***in lie &#8220;@LifeAsKing: @2...\n",
       "56741         1  you've gone and broke the wrong heart baby, an...\n",
       "56742         1  young buck wanna eat!!.. dat nigguh like I ain...\n",
       "56743         1              youu got wild bitches tellin you lies\n",
       "56744         0  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n",
       "\n",
       "[56745 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.drop(columns=['Unnamed: 0'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a7acd0c-60f1-4896-8f1c-5d5eaa86ac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\JY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  \\\n",
      "0   @user when a father is dysfunctional and is s...   \n",
      "1  @user @user thanks for #lyft credit i can't us...   \n",
      "2                                bihday your majesty   \n",
      "3  #model   i love u take with u all the time in ...   \n",
      "4             factsguide: society now    #motivation   \n",
      "\n",
      "                                       cleaned_tweet  \n",
      "0  father dysfunctional selfish drag kid dysfunct...  \n",
      "1  thanks lyft credit cant use cause dont offer w...  \n",
      "2                                     bihday majesty  \n",
      "3  model love u take u time urð± ðððð...  \n",
      "4                      factsguide society motivation  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    # Remove user @ references and '#' from hashtags\n",
    "    text = re.sub(r'\\@\\w+|\\#\\ð\\\\±','', text)\n",
    "    # Remove punctuations\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    # Remove stop words and lemmatize\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the tweet column\n",
    "df['cleaned_tweet'] = df['tweet'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows of the DataFrame after preprocessing\n",
    "print(df[['tweet', 'cleaned_tweet']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6eb0ac82-806c-4020-be4c-0f82cb28174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaaa</th>\n",
       "      <th>aaaaaaaaand</th>\n",
       "      <th>aaaaaand</th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>aaahh</th>\n",
       "      <th>aaahhhh</th>\n",
       "      <th>aaahhhhh</th>\n",
       "      <th>...</th>\n",
       "      <th>zydeco</th>\n",
       "      <th>zz</th>\n",
       "      <th>zzz</th>\n",
       "      <th>zzzquil</th>\n",
       "      <th>zzzzzz</th>\n",
       "      <th>zzzzzzzz</th>\n",
       "      <th>zã¼rich</th>\n",
       "      <th>ë¹</th>\n",
       "      <th>ðµð¹</th>\n",
       "      <th>ó¾</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41710 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaa  aaaaa  aaaaaaaaand  aaaaaand  aaaaah  aaaaand  aaahh  aaahhhh  \\\n",
       "0   0    0      0            0         0       0        0      0        0   \n",
       "1   0    0      0            0         0       0        0      0        0   \n",
       "2   0    0      0            0         0       0        0      0        0   \n",
       "3   0    0      0            0         0       0        0      0        0   \n",
       "4   0    0      0            0         0       0        0      0        0   \n",
       "\n",
       "   aaahhhhh  ...  zydeco  zz  zzz  zzzquil  zzzzzz  zzzzzzzz  zã¼rich  ë¹  \\\n",
       "0         0  ...       0   0    0        0       0         0        0   0   \n",
       "1         0  ...       0   0    0        0       0         0        0   0   \n",
       "2         0  ...       0   0    0        0       0         0        0   0   \n",
       "3         0  ...       0   0    0        0       0         0        0   0   \n",
       "4         0  ...       0   0    0        0       0         0        0   0   \n",
       "\n",
       "   ðµð¹  ó¾  \n",
       "0     0   0  \n",
       "1     0   0  \n",
       "2     0   0  \n",
       "3     0   0  \n",
       "4     0   0  \n",
       "\n",
       "[5 rows x 41710 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stop words, stemmer, and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example preprocessing function\n",
    "def preprocess(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stop words, then stem and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(stemmer.stem(word)) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the text data\n",
    "df['cleaned_tweet'] = df['cleaned_tweet'].apply(preprocess)\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "sparse_matrix = vectorizer.fit_transform(df['cleaned_tweet'])\n",
    "\n",
    "# Convert sparse matrix to a sparse DataFrame (sparse is for only store non-zero elements, saving significant memory.)\n",
    "output = pd.DataFrame.sparse.from_spmatrix(sparse_matrix, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Assign target variable\n",
    "y = df['Toxicity']\n",
    "\n",
    "output.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e0f260c-7a9a-4cc4-a8d0-28c7c1f46ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\JY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********LogisticRegression********\n",
      "Train Accuracy: 0.93825\n",
      "Train Precision: 0.9812903225806452\n",
      "Train Recall: 0.8746405980448534\n",
      "Train F1: 0.924901185770751\n",
      "Test Accuracy: 0.9035\n",
      "Test Precision: 0.9685714285714285\n",
      "Test Recall: 0.7985865724381626\n",
      "Test F1: 0.8754034861200775\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92      1151\n",
      "           1       0.97      0.80      0.88       849\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.92      0.89      0.90      2000\n",
      "weighted avg       0.91      0.90      0.90      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1129   22]\n",
      " [ 171  678]]\n",
      "ROC-AUC Score: 0.9720031436790254\n",
      "********KNeighborsClassifier********\n",
      "Train Accuracy: 0.618625\n",
      "Train Precision: 0.9591397849462365\n",
      "Train Recall: 0.1282346175963197\n",
      "Train F1: 0.22622368754755262\n",
      "Test Accuracy: 0.617\n",
      "Test Precision: 0.946236559139785\n",
      "Test Recall: 0.10365135453474676\n",
      "Test F1: 0.18683651804670912\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75      1151\n",
      "           1       0.95      0.10      0.19       849\n",
      "\n",
      "    accuracy                           0.62      2000\n",
      "   macro avg       0.77      0.55      0.47      2000\n",
      "weighted avg       0.75      0.62      0.51      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1146    5]\n",
      " [ 761   88]]\n",
      "ROC-AUC Score: 0.6128598166801235\n",
      "********SVC********\n",
      "Train Accuracy: 0.99275\n",
      "Train Precision: 0.9976717112922002\n",
      "Train Recall: 0.9856239217941346\n",
      "Train F1: 0.9916112236042812\n",
      "Test Accuracy: 0.911\n",
      "Test Precision: 0.9718706047819972\n",
      "Test Recall: 0.8138987043580683\n",
      "Test F1: 0.8858974358974359\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93      1151\n",
      "           1       0.97      0.81      0.89       849\n",
      "\n",
      "    accuracy                           0.91      2000\n",
      "   macro avg       0.92      0.90      0.91      2000\n",
      "weighted avg       0.92      0.91      0.91      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1131   20]\n",
      " [ 158  691]]\n",
      "ROC-AUC Score: 0.9728463700842919\n",
      "********DecisionTreeClassifier********\n",
      "Train Accuracy: 0.999875\n",
      "Train Precision: 0.9997125610807703\n",
      "Train Recall: 1.0\n",
      "Train F1: 0.9998562598821331\n",
      "Test Accuracy: 0.916\n",
      "Test Precision: 0.9157509157509157\n",
      "Test Recall: 0.8833922261484098\n",
      "Test F1: 0.8992805755395683\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      1151\n",
      "           1       0.92      0.88      0.90       849\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.92      0.91      0.91      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1082   69]\n",
      " [  99  750]]\n",
      "ROC-AUC Score: 0.9120706222581071\n",
      "********RandomForestClassifier********\n",
      "Train Accuracy: 0.999875\n",
      "Train Precision: 0.9997125610807703\n",
      "Train Recall: 1.0\n",
      "Train F1: 0.9998562598821331\n",
      "Test Accuracy: 0.9185\n",
      "Test Precision: 0.92875\n",
      "Test Recall: 0.8751472320376914\n",
      "Test F1: 0.9011522134627047\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93      1151\n",
      "           1       0.93      0.88      0.90       849\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.92      0.91      0.92      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1094   57]\n",
      " [ 106  743]]\n",
      "ROC-AUC Score: 0.9703202725340487\n",
      "********MultinomialNB********\n",
      "Train Accuracy: 0.95425\n",
      "Train Precision: 0.9346368715083799\n",
      "Train Recall: 0.9620471535365153\n",
      "Train F1: 0.9481439501275148\n",
      "Test Accuracy: 0.895\n",
      "Test Precision: 0.8553948832035595\n",
      "Test Recall: 0.9057714958775029\n",
      "Test F1: 0.879862700228833\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91      1151\n",
      "           1       0.86      0.91      0.88       849\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.89      0.90      0.89      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1021  130]\n",
      " [  80  769]]\n",
      "ROC-AUC Score: 0.9611138570547043\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Load the dataset (assuming it's already loaded into `df`)\n",
    "# Sample a subset of the data\n",
    "df_subset = df.sample(n=10000, random_state=42)\n",
    "\n",
    "# Apply preprocessing to the subset\n",
    "df_subset['cleaned_tweet'] = df_subset['tweet'].apply(preprocess_text)\n",
    "\n",
    "# Feature Extraction using TF-IDF with limited vocabulary size\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df_subset['cleaned_tweet'])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df_subset['Toxicity'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = [LogisticRegression(),\n",
    "          KNeighborsClassifier(),\n",
    "          SVC(probability=True),\n",
    "          DecisionTreeClassifier(),\n",
    "          RandomForestClassifier(),\n",
    "          MultinomialNB()]\n",
    "\n",
    "# Train and evaluate models\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)  \n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    test_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else test_pred\n",
    "\n",
    "    print(f\"********{type(model).__name__}********\")\n",
    "    print(f\"Train Accuracy: {accuracy_score(y_train, train_pred)}\")\n",
    "    print(f\"Train Precision: {precision_score(y_train, train_pred)}\")\n",
    "    print(f\"Train Recall: {recall_score(y_train, train_pred)}\")\n",
    "    print(f\"Train F1: {f1_score(y_train, train_pred)}\")\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy_score(y_test, test_pred)}\")\n",
    "    print(f\"Test Precision: {precision_score(y_test, test_pred)}\")\n",
    "    print(f\"Test Recall: {recall_score(y_test, test_pred)}\")\n",
    "    print(f\"Test F1: {f1_score(y_test, test_pred)}\\n\\n\")\n",
    "\n",
    "    # Evaluate the Model\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, test_pred))\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, test_pred))\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_test, test_prob)\n",
    "    print(f\"ROC-AUC Score: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc8ec280-6e3e-49d0-b6ed-d9b7a657458a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\JY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********LogisticRegression********\n",
      "Train Accuracy: 0.97775\n",
      "Train Precision: 0.9925373134328358\n",
      "Train Recall: 0.9560092006900518\n",
      "Train F1: 0.9739308728763914\n",
      "Test Accuracy: 0.925\n",
      "Test Precision: 0.9568627450980393\n",
      "Test Recall: 0.8621908127208481\n",
      "Test F1: 0.9070631970260223\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      1151\n",
      "           1       0.96      0.86      0.91       849\n",
      "\n",
      "    accuracy                           0.93      2000\n",
      "   macro avg       0.93      0.92      0.92      2000\n",
      "weighted avg       0.93      0.93      0.92      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1118   33]\n",
      " [ 117  732]]\n",
      "ROC-AUC Score: 0.9719340687004386\n",
      "********KNeighborsClassifier********\n",
      "Train Accuracy: 0.901625\n",
      "Train Precision: 0.9722709722709723\n",
      "Train Recall: 0.7964347326049454\n",
      "Train F1: 0.875612454559823\n",
      "Test Accuracy: 0.8765\n",
      "Test Precision: 0.963076923076923\n",
      "Test Recall: 0.7373380447585395\n",
      "Test F1: 0.8352234823215477\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.98      0.90      1151\n",
      "           1       0.96      0.74      0.84       849\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.90      0.86      0.87      2000\n",
      "weighted avg       0.89      0.88      0.87      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1127   24]\n",
      " [ 223  626]]\n",
      "ROC-AUC Score: 0.9103800761155099\n",
      "********SVC********\n",
      "Train Accuracy: 0.9775\n",
      "Train Precision: 0.9948979591836735\n",
      "Train Recall: 0.9531339850488787\n",
      "Train F1: 0.973568281938326\n",
      "Test Accuracy: 0.9075\n",
      "Test Precision: 0.967605633802817\n",
      "Test Recall: 0.8091872791519434\n",
      "Test F1: 0.8813341885824246\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92      1151\n",
      "           1       0.97      0.81      0.88       849\n",
      "\n",
      "    accuracy                           0.91      2000\n",
      "   macro avg       0.92      0.89      0.90      2000\n",
      "weighted avg       0.91      0.91      0.91      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1128   23]\n",
      " [ 162  687]]\n",
      "ROC-AUC Score: 0.9685698614100096\n",
      "********DecisionTreeClassifier********\n",
      "Train Accuracy: 0.999875\n",
      "Train Precision: 0.9997125610807703\n",
      "Train Recall: 1.0\n",
      "Train F1: 0.9998562598821331\n",
      "Test Accuracy: 0.915\n",
      "Test Precision: 0.9027283511269276\n",
      "Test Recall: 0.8963486454652533\n",
      "Test F1: 0.8995271867612293\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93      1151\n",
      "           1       0.90      0.90      0.90       849\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.91      0.91      0.91      2000\n",
      "weighted avg       0.91      0.92      0.91      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1069   82]\n",
      " [  88  761]]\n",
      "ROC-AUC Score: 0.9144278698606938\n",
      "********RandomForestClassifier********\n",
      "Train Accuracy: 0.999875\n",
      "Train Precision: 0.9997125610807703\n",
      "Train Recall: 1.0\n",
      "Train F1: 0.9998562598821331\n",
      "Test Accuracy: 0.9245\n",
      "Test Precision: 0.9225181598062954\n",
      "Test Recall: 0.8975265017667845\n",
      "Test F1: 0.9098507462686567\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94      1151\n",
      "           1       0.92      0.90      0.91       849\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.92      0.92      0.92      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1087   64]\n",
      " [  87  762]]\n",
      "ROC-AUC Score: 0.9714438921857267\n",
      "********MultinomialNB********\n",
      "Train Accuracy: 0.946125\n",
      "Train Precision: 0.9096531325625168\n",
      "Train Recall: 0.9726854514088557\n",
      "Train F1: 0.9401139363623732\n",
      "Test Accuracy: 0.8835\n",
      "Test Precision: 0.8228511530398323\n",
      "Test Recall: 0.9246171967020024\n",
      "Test F1: 0.8707709373266778\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.85      0.89      1151\n",
      "           1       0.82      0.92      0.87       849\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.88      0.89      0.88      2000\n",
      "weighted avg       0.89      0.88      0.88      2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[982 169]\n",
      " [ 64 785]]\n",
      "ROC-AUC Score: 0.9648556742280743\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Assuming df is your DataFrame and contains the tweet data\n",
    "# Sample a subset of the data\n",
    "df_subset = df.sample(n=10000, random_state=42)\n",
    "\n",
    "# Apply preprocessing to the subset\n",
    "df_subset['cleaned_tweet'] = df_subset['tweet'].apply(preprocess_text)\n",
    "\n",
    "# Feature Extraction using CountVectorizer (Bag of Words) with limited vocabulary size\n",
    "count_vectorizer = CountVectorizer(max_features=10000)\n",
    "X_bow = count_vectorizer.fit_transform(df_subset['cleaned_tweet'])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, df_subset['Toxicity'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = [LogisticRegression(),\n",
    "          KNeighborsClassifier(),\n",
    "          SVC(probability=True),\n",
    "          DecisionTreeClassifier(),\n",
    "          RandomForestClassifier(),\n",
    "          MultinomialNB()]\n",
    "\n",
    "# Train and evaluate models\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)  \n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    test_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else test_pred\n",
    "\n",
    "    print(f\"********{type(model).__name__}********\")\n",
    "    print(f\"Train Accuracy: {accuracy_score(y_train, train_pred)}\")\n",
    "    print(f\"Train Precision: {precision_score(y_train, train_pred)}\")\n",
    "    print(f\"Train Recall: {recall_score(y_train, train_pred)}\")\n",
    "    print(f\"Train F1: {f1_score(y_train, train_pred)}\")\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy_score(y_test, test_pred)}\")\n",
    "    print(f\"Test Precision: {precision_score(y_test, test_pred)}\")\n",
    "    print(f\"Test Recall: {recall_score(y_test, test_pred)}\")\n",
    "    print(f\"Test F1: {f1_score(y_test, test_pred)}\\n\\n\")\n",
    "\n",
    "    # Evaluate the Model\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, test_pred))\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, test_pred))\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_test, test_prob)\n",
    "    print(f\"ROC-AUC Score: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ed287-cd96-4c0a-a230-33dafc0c4b81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
